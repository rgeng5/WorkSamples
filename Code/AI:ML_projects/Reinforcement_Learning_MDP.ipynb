{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning: Markov-Decision Process"
      ],
      "metadata": {
        "id": "um0g_RGHcoZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MDP setup\n",
        "states = {'s1', 's2'}\n",
        "actions_1 = {'A','notA'}\n",
        "actions_2 = {'R','notR'}\n",
        "transition_probs = {'s1':{'A': {'s1': 0.8, 's2': 0.2},'notA': {'s1': 0.5, 's2': 0.5}},\n",
        "                    's2':{'R': {'s1': 0.7, 's2': 0.3},'notR': {'s1': 0.4, 's2': 0.6}}}\n",
        "rewards = {'s1': {'A': 4, 'notA': 6},'s2': {'R': -5, 'notR': -3}}"
      ],
      "metadata": {
        "id": "y4hao5u2IOaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration\n",
        "alpha = 0.99  # discount factor\n",
        "max_iter = 1000   # maximum iterations allowed\n",
        "threshold_diff = 0.0001  # stop if the difference between new values and \n",
        "#old values is smaller than this threshold\n",
        "\n",
        "# initialize\n",
        "state_values = {'s1': 0, 's2': 0}\n",
        "new_state_values = {'s1': 0, 's2': 0}\n",
        "\n",
        "for i in range(max_iter):\n",
        "\n",
        "  # Value function update\n",
        "  for s in states:\n",
        "    if s == 's1':\n",
        "      sum_set={'A': 0, 'notA': 0}\n",
        "      for a in actions_1:\n",
        "        sum=0\n",
        "        for new_s in states:\n",
        "          tmp=transition_probs[s][a][new_s]*(rewards[s][a]+alpha*state_values[new_s])\n",
        "          sum+=tmp\n",
        "        sum_set[a]=sum\n",
        "      new_state_values[s]=max(sum_set.values())\n",
        "    \n",
        "    if s == 's2':\n",
        "      sum_set={'R': 0, 'notR': 0}\n",
        "      for a in actions_2:\n",
        "        sum=0\n",
        "        for new_s in states:\n",
        "          tmp=transition_probs[s][a][new_s]*(rewards[s][a]+alpha*state_values[new_s])\n",
        "          sum+=tmp\n",
        "        sum_set[a]=sum\n",
        "      new_state_values[s]=max(sum_set.values())\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(\"Value function for iteration\", i)\n",
        "    print(new_state_values)\n",
        "\n",
        "  diff = max(abs(new_state_values[s] - state_values[s]) for s in states)\n",
        "  for s in states:\n",
        "    state_values[s]= new_state_values[s] \n",
        "    \n",
        "  # Extract optimal policy\n",
        "  if diff < threshold_diff:\n",
        "    print(\"Terminate.\")\n",
        "    print(\"The optimal value function is:\")\n",
        "    print(state_values)\n",
        "    policy={'s1': '', 's2':''}\n",
        "    for s in states:\n",
        "      if s == 's1':\n",
        "        sum_set={'A': 0, 'notA': 0}\n",
        "        for a in actions_1:\n",
        "          sum=0\n",
        "          for new_s in states:\n",
        "            tmp=transition_probs[s][a][new_s]*(rewards[s][a]+alpha*state_values[new_s])\n",
        "            sum+=tmp\n",
        "          sum_set[a]=sum\n",
        "        policy[s]=max(sum_set, key=sum_set.get)\n",
        "      \n",
        "      if s == 's2':\n",
        "        sum_set={'R': 0, 'notR': 0}\n",
        "        for a in actions_2:\n",
        "          sum=0\n",
        "          for new_s in states:\n",
        "            tmp=transition_probs[s][a][new_s]*(rewards[s][a]+alpha*state_values[new_s])\n",
        "            sum+=tmp\n",
        "          sum_set[a]=sum\n",
        "        policy[s]=max(sum_set, key=sum_set.get)\n",
        "    print(\"The optimal policy is:\")\n",
        "    print(policy)\n",
        "    break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nEcwc8IuTiz",
        "outputId": "c1d2365f-9883-4452-c354-af9ff616c79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value function for iteration 0\n",
            "{'s1': 6.0, 's2': -3.0}\n",
            "Value function for iteration 100\n",
            "{'s1': 130.47741693730623, 's2': 120.4885157164405}\n",
            "Value function for iteration 200\n",
            "{'s1': 175.95973955469884, 's2': 165.97083833383311}\n",
            "Value function for iteration 300\n",
            "{'s1': 192.60774058888745, 's2': 182.61883936802175}\n",
            "Value function for iteration 400\n",
            "{'s1': 198.7014473849508, 's2': 188.71254616408507}\n",
            "Value function for iteration 500\n",
            "{'s1': 200.93194115054646, 's2': 190.9430399296807}\n",
            "Value function for iteration 600\n",
            "{'s1': 201.74837400576274, 's2': 191.759472784897}\n",
            "Value function for iteration 700\n",
            "{'s1': 202.04721483524995, 's2': 192.0583136143842}\n",
            "Value function for iteration 800\n",
            "{'s1': 202.15660024373523, 's2': 192.1676990228695}\n",
            "Value function for iteration 900\n",
            "{'s1': 202.19663884090426, 's2': 192.20773762003853}\n",
            "Terminate.\n",
            "The optimal value function is:\n",
            "{'s1': 202.2099174659546, 's2': 192.22101624508883}\n",
            "The optimal policy is:\n",
            "{'s1': 'A', 's2': 'R'}\n"
          ]
        }
      ]
    }
  ]
}